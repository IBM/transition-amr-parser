"""A middle layer (we call Reformer) that serves in between the actions generated from the amr oracle
and the actions in the modeling:

original action sequence (from oracle) -> reformer -> action sequence to be conceived by the model
                                                      (+ any state information at each token step)
action sequence generated by the model -> reformer -> state information at each token step

Usage:
- At training time, the actions (input and output to the decoder) along with any state information are generated in
preprocessing.
- At decoding time, the reformer runs with the reformulated action sequence, and generate any state information on
the fly.

Here the reformer deals with rare node action spliting into subtokens.
"""

from transition_amr_parser.o10_amr_machine import AMRStateMachine, peel_pointer, arc_nopointer_regex
from fairseq_ext.utils import join_action_pointer


class AMRActionReformerSubtok:
    """Reformer of the action sequence.

    Given the original action sequence from the oracle, reformulate it to the sequence for the desired form for our
    particular Transformer model input and output. It includes altering:
    - the input action sequence tokens
    - the pointer values
    - all the states related that need to be input to the model

    For model decoding, the reverse process is also done here.
    """
    def __init__(self, dictionary=None, machine_config=None):
        self.dictionary = dictionary
        self.INIT = dictionary.bpe.INIT

        self.machine_config = machine_config
        self.machine = AMRStateMachine.from_config(machine_config)

    @staticmethod
    def reform_actions_and_get_states(tokens, actions, dictionary, machine):
        """At training time, formulate the target input and output and pointers, as well as state information at
        each step used by the model.
        Running stateless (vs. at decoding time, it's stateful).

        Args:
            tokens (List[str]): source tokens.
            actions (List[str]): original actions; arc actions must include pointer values.
        """
        # reset state machine with new tokens
        machine.reset(tokens)

        # model decoder input and output
        actions_nopos_in = []             # not shifted
        actions_nopos_out = []
        actions_pos = []                  # associated with `actions_nopos_out`

        # state information to be used
        token_cursors = []
        allowed_base_actions = []
        actions_nodemask = []

        # facilitating information
        tok_to_subtok_start = []    # map from token index to subtoken start position index
        subtok_origin_index = []    # the index of the original token for each subtoken
        num_subtok = 0    # number of subtokens in total
        subtok_start_mask = []    # mark the start of each subtoken sequences

        for i, act in enumerate(actions):
            # first need to split out the pointers
            act, pos = peel_pointer(act, pad=-1)

            # tokenize the action
            sub_toks = dictionary.tokenize_act(act)

            # update counting and internal mapping information
            tok_to_subtok_start.append(num_subtok)
            subtok_origin_index += [i] * len(sub_toks)
            num_subtok += len(sub_toks)
            subtok_start_mask += ([1] + [0] * (len(sub_toks) - 1))

            # reformulate pointer position due to action token split
            pos = tok_to_subtok_start[pos] if pos != -1 else pos

            # decoder input and output
            # symbols in `sub_toks` are those in the bpe vocabulary, e.g. ['Ġwant-01']
            actions_nopos_in += sub_toks
            actions_nopos_out += sub_toks
            actions_pos.append(pos)

            # action word not splitted
            if len(sub_toks) == 1:

                # state information
                token_cursors.append(machine.tok_cursor)
                act_allowed = machine.get_valid_actions(max_1root=True)
                base_act = machine.get_base_action(act)
                assert base_act in act_allowed, 'current action not in the allowed space? check the rules.'
                allowed_base_actions.append(act_allowed)
                machine.update(act)

            # action word is splitted
            else:

                # state information
                token_cursors.extend([machine.tok_cursor] * len(sub_toks))
                act_allowed = machine.get_valid_actions(max_1root=True)
                base_act = machine.get_base_action(act)
                assert base_act in act_allowed, 'current action not in the allowed space? check the rules.'
                allowed_base_actions.extend([act_allowed] * len(sub_toks))
                machine.update(act)

        assert len(tok_to_subtok_start) == len(actions)
        assert len(subtok_origin_index) == num_subtok

        actions_nodemask_ori = machine.get_actions_nodemask()
        actions_nodemask = [actions_nodemask_ori[idx] for idx in subtok_origin_index]
        # only allow begin of subtokens to be pointed to
        actions_nodemask = [x if y == 1 else 0 for x, y in zip(actions_nodemask, subtok_start_mask)]

        return {'actions_nopos_in': actions_nopos_in,
                'actions_nopos_out': actions_nopos_out,
                'actions_pos': actions_pos,
                # general states
                'allowed_base_actions': allowed_base_actions,
                'token_cursors': token_cursors,
                'actions_nodemask': actions_nodemask
                }

    def reset(self, tokens):
        self.machine.reset(tokens)
        # model decoding input and output
        self.actions_nopos_in = []
        self.actions_nopos_out = []
        self.actions_pos = []
        # general state information
        self.allowed_base_actions = []
        self.token_cursors = [0]    # src token cursor BEFORE each action
        self.actions_nodemask = []

        # step counter
        self.machine_step = 0    # only on complete actions
        self.action_step = 0     # on all actions including subtokens

        # internal states to facilitate decoding
        self.subtok_origin_index = []    # the index of the original token for each subtoken
        self.subtok_start_mask = []    # mark the start of each subtoken sequences

    def get_states(self):
        # return state info to be used to model
        act_allowed = self.machine.get_valid_actions(max_1root=True)
        # NOTE here we do not force start of new token at the right places, e.g. after SHIFT, it should be a start

        states = {'actions_nopos_in': self.actions_nopos_in,
                  'actions_nopos_out': self.actions_nopos_out,
                  'actions_pos': self.actions_pos,
                  # general states
                  'allowed_base_actions': act_allowed,    # NOTE here it is just for one step, instead of accumulated
                  'token_cursors': self.token_cursors,
                  'actions_nodemask': self.actions_nodemask
                  }
        return states

    def apply_action_and_update_states(self, action_nopos, action_pos):
        """At decoding time, apply an action decoded by model (action without pointer + the decoded pointer value), and
        update the state information so far to be used for next step decoding.

        Args:
            action_nopos (str): action decoded as direct model output (mapped to str by model dictionary)
            action_pos (int): pointer value decoded by model
        """
        assert getattr(self.machine, 'tokens', None) is not None, 'must initialize the Reformer with src tokens ' \
            'at decoding time to apply actions step by step'

        self.actions_nopos_in.append(action_nopos)    # focuses on the input for next decoding step: need to be shifted
        self.actions_nopos_out.append(action_nopos)    # non-shifted version
        self.actions_pos.append(action_pos)    # non-shifted version

        if action_nopos.startswith(self.INIT):
            # start of a new action
            self.machine.update(join_action_pointer(action_nopos[1:], action_pos))
            self.machine_step += 1
            self.subtok_start_mask.append(1)
        else:
            # inside an action (only node action now): do nothing, just wait
            self.subtok_start_mask.append(0)

        self.subtok_origin_index.append(self.machine_step - 1)

        self.token_cursors.append(self.machine.tok_cursor)

        actions_nodemask_ori = self.machine.get_actions_nodemask()
        self.actions_nodemask = [actions_nodemask_ori[idx] for idx in self.subtok_origin_index]
        # only allow begin of subtokens to be pointed to
        self.actions_nodemask = [x if y == 1 else 0 for x, y in zip(self.actions_nodemask, self.subtok_start_mask)]

        self.action_step += 1

        return

    @staticmethod
    def recover_actions(actions_nopos, actions_pos, dictionary):
        """Recover the original actions corresponding to the oracle.
        Used in postprocessing after model decoding.

        Args:
            actions_nopos (List[str]): from model output, and translated back to str by dictionary, e.g. ['Ġwant-01']
            actions_pos (List[int]): from model output.
        """
        rec_actions_nopos = []
        rec_actions_pos = []
        rec_index_map = {}    # map of unmerged token position (only the beginning of words) to mapped token position
        current_act = ''
        for i, (act, pos) in enumerate(zip(actions_nopos, actions_pos)):
            # merge the splitted tokens
            if act.startswith(dictionary.bpe.INIT):
                if current_act:
                    rec_actions_nopos.append(current_act)
                current_act = act[1:]
                rec_index_map[i] = len(rec_actions_nopos)
            else:
                current_act += act

            if i == len(actions_nopos) - 1:
                assert current_act
                rec_actions_nopos.append(current_act)

            # deal with pointer values
            if act.startswith(dictionary.bpe.INIT):
                if arc_nopointer_regex.match(act[1:]):
                    rec_actions_pos.append(rec_index_map[pos])
                else:
                    rec_actions_pos.append(-1)

        assert len(rec_actions_nopos) == len(rec_actions_pos)
        rec_actions = [join_action_pointer(act, pos) for act, pos in zip(rec_actions_nopos, rec_actions_pos)]
        return rec_actions_nopos, rec_actions_pos, rec_actions
