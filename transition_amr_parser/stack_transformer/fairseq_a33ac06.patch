From cfb8785e7a4c7ade6bf9594b51d5b7c8e3a986f7 Mon Sep 17 00:00:00 2001
From: Ramon Astudillo <ramon.astudillo@ibm.com>
Date: Wed, 7 Oct 2020 23:04:49 -0400
Subject: [PATCH] Add stack-transformer v0.3.2 code

---
 .../label_smoothed_cross_entropy.py           |  20 +-
 fairseq/data/data_utils.py                    |   4 +-
 fairseq/data/dictionary.py                    |   4 +-
 fairseq/data/indexed_dataset.py               |  13 +-
 fairseq/data/language_pair_dataset.py         | 184 ++++++++-
 fairseq/models/fairseq_model.py               |   9 +-
 fairseq/models/transformer.py                 | 354 +++++++++++++++++-
 fairseq/modules/multihead_attention.py        | 103 ++++-
 fairseq/modules/transformer_layer.py          |   5 +
 fairseq/optim/adam.py                         |   3 +-
 fairseq/options.py                            |  38 ++
 fairseq/sequence_generator.py                 |  92 ++++-
 fairseq/tasks/fairseq_task.py                 |  43 ++-
 fairseq/tasks/translation.py                  |  72 +++-
 fairseq/tokenizer.py                          |   5 +
 fairseq/utils.py                              |  14 +-
 generate.py                                   |  36 +-
 preprocess.py                                 |  46 ++-
 train.py                                      |  21 +-
 19 files changed, 987 insertions(+), 79 deletions(-)

diff --git a/fairseq/criterions/label_smoothed_cross_entropy.py b/fairseq/criterions/label_smoothed_cross_entropy.py
index 66877187..64c90682 100644
--- a/fairseq/criterions/label_smoothed_cross_entropy.py
+++ b/fairseq/criterions/label_smoothed_cross_entropy.py
@@ -13,18 +13,26 @@ from . import FairseqCriterion, register_criterion
 def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=None, reduce=True):
     if target.dim() == lprobs.dim() - 1:
         target = target.unsqueeze(-1)
-    nll_loss = -lprobs.gather(dim=-1, index=target)
-    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)
+
+    # remove entries of ignored indices
     if ignore_index is not None:
-        non_pad_mask = target.ne(ignore_index)
-        nll_loss = nll_loss[non_pad_mask]
-        smooth_loss = smooth_loss[non_pad_mask]
-    else:
+        # FIXME: May have broken other cases in orther to make the smoothed
+        # loss suppor -Inf logits
+        assert lprobs.dim() == 2
+        lprobs = lprobs[target.ne(ignore_index).view(-1), :]
+        target = target[target.ne(ignore_index)].unsqueeze(1)
+
+    nll_loss = -lprobs.gather(dim=-1, index=target)
+    # support -Inf
+    smooth_loss = -lprobs[lprobs != float("-Inf")].sum(dim=-1, keepdim=True)
+    if ignore_index is None:
         nll_loss = nll_loss.squeeze(-1)
         smooth_loss = smooth_loss.squeeze(-1)
     if reduce:
         nll_loss = nll_loss.sum()
         smooth_loss = smooth_loss.sum()
+    else:
+        raise NotImplementedError("Suporting -Inf removed non reduce mode")
     eps_i = epsilon / lprobs.size(-1)
     loss = (1. - epsilon) * nll_loss + eps_i * smooth_loss
     return loss, nll_loss
diff --git a/fairseq/data/data_utils.py b/fairseq/data/data_utils.py
index 38c38501..94cdd9db 100644
--- a/fairseq/data/data_utils.py
+++ b/fairseq/data/data_utils.py
@@ -32,7 +32,9 @@ def collate_tokens(values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_be
     def copy_tensor(src, dst):
         assert dst.numel() == src.numel()
         if move_eos_to_beginning:
-            assert src[-1] == eos_idx
+            # FIXME: Unclear why this assert is necessary. We should not use
+            # last element anyway
+            # assert src[-1] == eos_idx
             dst[0] = eos_idx
             dst[1:] = src[:-1]
         else:
diff --git a/fairseq/data/dictionary.py b/fairseq/data/dictionary.py
index 417105e5..bf6c79ac 100644
--- a/fairseq/data/dictionary.py
+++ b/fairseq/data/dictionary.py
@@ -60,7 +60,7 @@ class Dictionary(object):
             return self.indices[sym]
         return self.unk_index
 
-    def string(self, tensor, bpe_symbol=None, escape_unk=False):
+    def string(self, tensor, bpe_symbol=None, escape_unk=False, split_token=' '):
         """Helper for converting a tensor of token indices to a string.
 
         Can optionally remove BPE symbols or escape <unk> words.
@@ -74,7 +74,7 @@ class Dictionary(object):
             else:
                 return self[i]
 
-        sent = ' '.join(token_string(i) for i in tensor if i != self.eos())
+        sent = split_token.join(token_string(i) for i in tensor if i != self.eos())
         return data_utils.process_bpe_symbol(sent, bpe_symbol)
 
     def unk_string(self, escape=False):
diff --git a/fairseq/data/indexed_dataset.py b/fairseq/data/indexed_dataset.py
index 12497989..3b039375 100644
--- a/fairseq/data/indexed_dataset.py
+++ b/fairseq/data/indexed_dataset.py
@@ -41,9 +41,11 @@ def infer_dataset_impl(path):
         return None
 
 
-def make_builder(out_file, impl, vocab_size=None):
+def make_builder(out_file, impl, vocab_size=None, dtype=None):
     if impl == 'mmap':
-        return MMapIndexedDatasetBuilder(out_file, dtype=__best_fitting_dtype(vocab_size))
+        if dtype is None:
+            dtype = __best_fitting_dtype(vocab_size)
+        return MMapIndexedDatasetBuilder(out_file, dtype=dtype)
     else:
         return IndexedDatasetBuilder(out_file)
 
@@ -86,7 +88,7 @@ dtypes = {
     3: np.int16,
     4: np.int32,
     5: np.int64,
-    6: np.float,
+    6: np.float32,
     7: np.double,
     8: np.uint16
 }
@@ -289,7 +291,7 @@ class IndexedDatasetBuilder(object):
         np.int16: 2,
         np.int32: 4,
         np.int64: 8,
-        np.float: 4,
+        np.float32: 4,
         np.double: 8
     }
 
@@ -473,7 +475,8 @@ class MMapIndexedDataset(torch.utils.data.Dataset):
     def __getitem__(self, i):
         ptr, size = self._index[i]
         np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=size, offset=ptr)
-        if self._index.dtype != np.int64:
+        # FIXME: This is barely improves over previous hack
+        if self._index.dtype != np.float32:
             np_array = np_array.astype(np.int64)
 
         return torch.from_numpy(np_array)
diff --git a/fairseq/data/language_pair_dataset.py b/fairseq/data/language_pair_dataset.py
index 5fc1371a..3f508653 100644
--- a/fairseq/data/language_pair_dataset.py
+++ b/fairseq/data/language_pair_dataset.py
@@ -7,11 +7,17 @@ import numpy as np
 import torch
 
 from . import data_utils, FairseqDataset
+from transition_amr_parser.stack_transformer.data_utils import (
+    collate_embeddings,
+    collate_target_masks,
+    collate_wp_idx,
+    collate_masks
+)
 
 
 def collate(
     samples, pad_idx, eos_idx, left_pad_source=True, left_pad_target=False,
-    input_feeding=True,
+    input_feeding=True, state_machine=True
 ):
     if len(samples) == 0:
         return {}
@@ -34,8 +40,13 @@ def collate(
     target = None
     if samples[0].get('target', None) is not None:
         target = merge('target', left_pad=left_pad_target)
+        # we will need for sanity check furtehr down
+        no_sorted_target = target.clone()
         target = target.index_select(0, sort_order)
-        ntokens = sum(len(s['target']) for s in samples)
+        # needed for sanity checks
+        tgt_legths = torch.LongTensor([len(s['target']) for s in samples])
+        ntokens = tgt_legths.sum().item()
+        tgt_legths = tgt_legths.index_select(0, sort_order)
 
         if input_feeding:
             # we create a shifted version of targets for feeding the
@@ -49,6 +60,100 @@ def collate(
     else:
         ntokens = sum(len(s['source']) for s in samples)
 
+    if samples[0].get('orig_tokens', None) is not None:
+        batch_orig_tokens = [s["orig_tokens"] for s in samples]
+        orig_tokens = []
+        for index in sort_order:
+            orig_tokens.append(batch_orig_tokens[index])
+    else:
+        orig_tokens = None
+
+    # Pre-trained embeddings
+    def merge_embeddings(key, left_pad_source, move_eos_to_beginning):
+        return collate_embeddings(
+            [s[key] for s in samples],
+            pad_idx, eos_idx, left_pad_source, move_eos_to_beginning
+        )
+
+    source_fix_emb = merge_embeddings(
+        'source_fix_emb',
+        left_pad_source,
+        False
+        #left_pad_target
+    )
+    source_fix_emb = source_fix_emb.index_select(0, sort_order)
+
+    # Word-pieces
+    src_wordpieces = merge('src_wordpieces', left_pad=left_pad_source)
+    src_wordpieces = src_wordpieces.index_select(0, sort_order)
+
+    def merge_wp_idx(key, left_pad, move_eos_to_beginning=False):
+        return collate_wp_idx(
+            [s[key] for s in samples],
+            pad_idx, eos_idx, left_pad, move_eos_to_beginning,
+            reverse=True
+        )
+
+    # Wordpiece to word mapping
+    src_wp2w = merge_wp_idx('src_wp2w', left_pad=left_pad_source)
+    src_wp2w = src_wp2w.index_select(0, sort_order)
+
+#     # DEBUG: Inline RoBERTa
+#     from torch_scatter import scatter_mean
+#     # extract roberta from collated
+#     roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')
+#     roberta.eval()
+#     last_layer = roberta.extract_features(src_wordpieces)
+#     # remove sentence start
+#     bsize, max_len, emb_size = last_layer.shape
+#     mask = (src_wordpieces != 0).unsqueeze(2).expand(last_layer.shape)
+#     last_layer = last_layer[mask].view((bsize, max_len - 1, emb_size))
+#     # remove sentence end
+#     last_layer = last_layer[:, :-1, :]
+#     # apply scatter, flip before to have left-side padding
+#     source_fix_emb2 = scatter_mean(last_layer, src_wp2w.unsqueeze(2), dim=1)
+#     source_fix_emb2 = source_fix_emb2.flip(1)
+#     # Remove extra padding
+#     source_fix_emb2 = source_fix_emb2[:, -src_tokens.shape[1]:, :]
+#     abs(source_fix_emb2 - source_fix_emb).max()
+#     # DEBUG: Inline RoBERTa
+
+    # source masks
+    def merge_masks(key, left_pad_source, left_pad_target):
+        return collate_masks(
+            [s[key] for s in samples],
+            pad_idx, eos_idx, left_pad_source, left_pad_target
+        )
+
+    # target masks
+    # get sub-set of active logits for this batch and mask for each individual
+    # sentence and target time step 
+    def merge_target_masks(left_pad_target):
+        return collate_target_masks(
+            [(s['target_masks'], s['active_logits'], len(s['target'])) for s in samples],
+            pad_idx, eos_idx, left_pad_target=left_pad_target, move_eos_to_beginning=False,
+            target=no_sorted_target
+        )
+
+    if state_machine:
+
+        # stack info
+        memory = merge_masks('memory', left_pad_source, left_pad_target)
+        memory = memory.index_select(0, sort_order)
+        memory_pos = merge_masks('memory_pos', left_pad_source, left_pad_target)
+        memory_pos = memory_pos.index_select(0, sort_order)
+        # active logits 
+        logits_mask, logits_indices = merge_target_masks(left_pad_target)
+        logits_mask = logits_mask.index_select(0, sort_order)
+
+    else:
+
+        memory = None
+        memory_pos = None
+        logits_indices = None
+        logits_mask = None
+
+    # batch variables
     batch = {
         'id': id,
         'nsentences': len(samples),
@@ -56,11 +161,24 @@ def collate(
         'net_input': {
             'src_tokens': src_tokens,
             'src_lengths': src_lengths,
+            'source_fix_emb': source_fix_emb,
+            'src_wordpieces': src_wordpieces, 
+            'src_wp2w': src_wp2w,
+            'memory': memory,
+            'memory_pos': memory_pos,
+            'logits_mask': logits_mask,
+            'logits_indices': logits_indices,
+            'orig_tokens': orig_tokens
         },
         'target': target,
     }
     if prev_output_tokens is not None:
         batch['net_input']['prev_output_tokens'] = prev_output_tokens
+
+    # sanity check batch
+    # from fairseq.debug_tools import sanity_check_collated_batch
+    # sanity_check_collated_batch(batch, pad_idx, left_pad_source, left_pad_target, tgt_legths)
+
     return batch
 
 
@@ -95,10 +213,18 @@ class LanguagePairDataset(FairseqDataset):
 
     def __init__(
         self, src, src_sizes, src_dict,
-        tgt=None, tgt_sizes=None, tgt_dict=None,
+        src_fix_emb, src_fix_emb_sizes,
+        src_wordpieces, src_wordpieces_sizes,
+        src_wp2w, src_wp2w_sizes,
+        tgt, tgt_sizes, tgt_dict,
+        memory, memory_sizes,
+        mem_pos, mem_pos_sizes,
+        target_masks, target_masks_sizes,
+        active_logits, active_logits_sizes,
         left_pad_source=True, left_pad_target=False,
         max_source_positions=1024, max_target_positions=1024,
         shuffle=True, input_feeding=True, remove_eos_from_source=False, append_eos_to_target=False,
+        state_machine=True
     ):
         if tgt_dict is not None:
             assert src_dict.pad() == tgt_dict.pad()
@@ -110,6 +236,22 @@ class LanguagePairDataset(FairseqDataset):
         self.tgt_sizes = np.array(tgt_sizes) if tgt_sizes is not None else None
         self.src_dict = src_dict
         self.tgt_dict = tgt_dict
+        # dataset variables
+        self.src_fix_emb = src_fix_emb
+        self.src_fix_emb_sizes = src_fix_emb_sizes
+        self.src_wordpieces = src_wordpieces
+        self.src_wordpieces_sizes = src_wordpieces_sizes
+        self.src_wp2w = src_wp2w
+        self.src_wp2w_sizes = src_wp2w_sizes
+        self.memory = memory
+        self.mem_pos = mem_pos
+        self.memory_sizes = np.array(memory_sizes)
+        self.mem_pos_sizes = np.array(mem_pos_sizes)
+        self.target_masks = target_masks
+        self.target_masks_sizes = np.array(target_masks_sizes)
+        self.active_logits = active_logits
+        self.active_logits_sizes = np.array(active_logits_sizes)
+        # other
         self.left_pad_source = left_pad_source
         self.left_pad_target = left_pad_target
         self.max_source_positions = max_source_positions
@@ -118,10 +260,31 @@ class LanguagePairDataset(FairseqDataset):
         self.input_feeding = input_feeding
         self.remove_eos_from_source = remove_eos_from_source
         self.append_eos_to_target = append_eos_to_target
+        # compute or not state of state machine
+        self.state_machine = state_machine
 
     def __getitem__(self, index):
         tgt_item = self.tgt[index] if self.tgt is not None else None
         src_item = self.src[index]
+
+        # Deduce pretrained embeddings size
+        pretrained_embed_dim = self.src_fix_emb[index].shape[0] // src_item.shape[0]
+        shape_factor = (self.src_fix_emb[index].shape[0] // pretrained_embed_dim, pretrained_embed_dim)
+        src_fix_emb_item = self.src_fix_emb[index].view(*shape_factor)
+        src_wordpieces_item = self.src_wordpieces[index]
+        src_wp2w_item = self.src_wp2w[index]
+        shape_factor = (tgt_item.shape[0], src_item.shape[0])
+        memory_item = self.memory[index].view(*shape_factor).transpose(0, 1)
+        memory_pos_item = self.mem_pos[index].view(*shape_factor).transpose(0, 1)
+        target_masks = self.target_masks[index]
+        active_logits = self.active_logits[index]
+
+        # Cast to float to simplify mask manipulation
+        memory_item = memory_item.type(src_fix_emb_item.type())
+        memory_pos_item = memory_pos_item.type(src_fix_emb_item.type())
+        target_masks = target_masks.type(src_fix_emb_item.type())
+        active_logits = active_logits.type(src_fix_emb_item.type())
+
         # Append EOS to end of tgt sentence if it does not have an EOS and remove
         # EOS from end of src sentence if it exists. This is useful when we use
         # use existing datasets for opposite directions i.e., when we want to
@@ -139,7 +302,14 @@ class LanguagePairDataset(FairseqDataset):
         return {
             'id': index,
             'source': src_item,
+            'source_fix_emb': src_fix_emb_item,
+            'src_wordpieces': src_wordpieces_item, 
+            'src_wp2w': src_wp2w_item,
             'target': tgt_item,
+            'memory': memory_item,
+            'memory_pos': memory_pos_item,
+            'target_masks': target_masks,
+            'active_logits': active_logits
         }
 
     def __len__(self):
@@ -178,6 +348,7 @@ class LanguagePairDataset(FairseqDataset):
             samples, pad_idx=self.src_dict.pad(), eos_idx=self.src_dict.eos(),
             left_pad_source=self.left_pad_source, left_pad_target=self.left_pad_target,
             input_feeding=self.input_feeding,
+            state_machine=self.state_machine
         )
 
     def num_tokens(self, index):
@@ -205,10 +376,17 @@ class LanguagePairDataset(FairseqDataset):
     def supports_prefetch(self):
         return (
             getattr(self.src, 'supports_prefetch', False)
+            and getattr(self.src_fix_emb, 'supports_prefetch', False)
             and (getattr(self.tgt, 'supports_prefetch', False) or self.tgt is None)
+            and getattr(self.memory, 'supports_prefetch', False)
+            and getattr(self.mem_pos, 'supports_prefetch', False)
+
         )
 
     def prefetch(self, indices):
         self.src.prefetch(indices)
         if self.tgt is not None:
             self.tgt.prefetch(indices)
+        self.src_fix_emb.prefetch(indices)
+        self.memory.prefetch(indices)
+        self.mem_pos.prefetch(indices)
diff --git a/fairseq/models/fairseq_model.py b/fairseq/models/fairseq_model.py
index fc53a7c9..bd591fbe 100644
--- a/fairseq/models/fairseq_model.py
+++ b/fairseq/models/fairseq_model.py
@@ -232,7 +232,14 @@ class FairseqEncoderDecoderModel(BaseFairseqModel):
                 - a dictionary with any model-specific outputs
         """
         encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)
-        features = self.decoder.extract_features(prev_output_tokens, encoder_out=encoder_out, **kwargs)
+        # FIXME: Because this abstraction at this level is so constraining, we
+        # need to hack removing unwanted arguments from the decoder extra
+        # arguments at this high level of abstraction. 
+        filtered_kwargs = {
+            key: value for key, value in kwargs
+            if key not in ['src_wordpieces', 'src_wp2w']
+        }
+        features = self.decoder.extract_features(prev_output_tokens, encoder_out=encoder_out, **filtered_kwargs)
         return features
 
     def output_layer(self, features, **kwargs):
diff --git a/fairseq/models/transformer.py b/fairseq/models/transformer.py
index c9ba5370..b8341ef6 100644
--- a/fairseq/models/transformer.py
+++ b/fairseq/models/transformer.py
@@ -26,6 +26,11 @@ from fairseq.modules import (
     TransformerEncoderLayer,
 )
 
+from transition_amr_parser.stack_transformer.stack_state_machine import (
+    state_machine_encoder
+)
+
+
 DEFAULT_MAX_SOURCE_POSITIONS = 1024
 DEFAULT_MAX_TARGET_POSITIONS = 1024
 
@@ -122,6 +127,19 @@ class TransformerModel(FairseqEncoderDecoderModel):
                                  'Must be used with adaptive_loss criterion'),
         parser.add_argument('--adaptive-softmax-dropout', type=float, metavar='D',
                             help='sets adaptive softmax dropout for the tail projections')
+        # Use stack transformer
+        parser.add_argument('--encode-state-machine', type=bool,
+                            help='controls encoding of stack and buffer')
+        # control BERT backprop
+        parser.add_argument('--bert-backprop', action='store_true',
+                            help='Backpropagate through BERT', default=False)
+        parser.add_argument('--no-bert-precompute', action='store_true',
+                            help='Compute BERT on the fly (debugging)',
+                            default=False)
+        parser.add_argument('--pretrained-embed-dim', type=int,
+                            help='Peetrained embeddings size',
+                            default=768)
+
         # fmt: on
 
     @classmethod
@@ -199,11 +217,26 @@ class TransformerEncoder(FairseqEncoder):
         self.register_buffer('version', torch.Tensor([3]))
 
         self.dropout = args.dropout
+        self.bert_backprop = args.bert_backprop
+        self.no_bert_precompute = args.no_bert_precompute
+
+        # backprop needs on the fly extraction
+        if self.bert_backprop or self.no_bert_precompute:
+            roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')
+            roberta.cuda()
+            self.roberta = roberta
+            # if args.no_bert_precompute:
+            #    # Set BERT to purely evaluation mode
+            #    self.roberta.eval()
 
         embed_dim = embed_tokens.embedding_dim
         self.padding_idx = embed_tokens.padding_idx
         self.max_source_positions = args.max_source_positions
 
+        # BERT embeddings as input
+        input_embed_dim = args.pretrained_embed_dim
+        self.subspace = Linear(input_embed_dim, embed_dim, bias=False)
+
         self.embed_tokens = embed_tokens
         self.embed_scale = math.sqrt(embed_dim)
         self.embed_positions = PositionalEmbedding(
@@ -222,7 +255,7 @@ class TransformerEncoder(FairseqEncoder):
         else:
             self.layer_norm = None
 
-    def forward(self, src_tokens, src_lengths):
+    def forward(self, src_tokens, src_lengths, memory, memory_pos, source_fix_emb, src_wordpieces, src_wp2w, **unused):
         """
         Args:
             src_tokens (LongTensor): tokens in the source language of shape
@@ -237,10 +270,50 @@ class TransformerEncoder(FairseqEncoder):
                 - **encoder_padding_mask** (ByteTensor): the positions of
                   padding elements of shape `(batch, src_len)`
         """
+
         # embed tokens and positions
-        x = self.embed_scale * self.embed_tokens(src_tokens)
-        if self.embed_positions is not None:
-            x += self.embed_positions(src_tokens)
+        # x = self.embed_scale * self.embed_tokens(src_tokens)
+        # if self.embed_positions is not None:
+        #     x += self.embed_positions(src_tokens)
+
+        if self.bert_backprop or self.no_bert_precompute:
+
+            # This is moved here since it gives impor errors. Wont be installed
+            # by default
+            from torch_scatter import scatter_mean
+
+            # extract roberta on the fly
+            last_layer = self.roberta.extract_features(src_wordpieces)
+            # remove sentence start
+            bsize, max_len, emb_size = last_layer.shape
+            mask = (src_wordpieces != 0).unsqueeze(2).expand(last_layer.shape)
+            last_layer = last_layer[mask].view((bsize, max_len - 1, emb_size))
+            # remove sentence end
+            last_layer = last_layer[:, :-1, :]
+            # apply scatter, src_wp2w was inverted in pre-processing to use
+            # scatter's left side padding . We need to flip the result.
+            source_fix_emb2 = scatter_mean(
+                last_layer,
+                src_wp2w.unsqueeze(2),
+                dim=1
+            )
+            source_fix_emb2 = source_fix_emb2.flip(1)
+            # Remove extra padding
+            source_fix_emb2 = source_fix_emb2[:, -src_tokens.shape[1]:, :]
+
+            # do not backprop for on-the-fly computing
+            if self.no_bert_precompute:
+                bert_embeddings = source_fix_emb2.detach()
+            else:
+                bert_embeddings = source_fix_emb2
+
+            # DEBUG: check precomputed and on the fly sufficiently close
+            # abs(source_fix_emb2 - source_fix_emb).max()
+        else:
+            # use pre-extracted roberta
+            bert_embeddings = source_fix_emb
+
+        x = self.subspace(bert_embeddings)
         x = F.dropout(x, p=self.dropout, training=self.training)
 
         # B x T x C -> T x B x C
@@ -338,6 +411,16 @@ class TransformerDecoder(FairseqIncrementalDecoder):
         self.embed_tokens = embed_tokens
         self.embed_scale = math.sqrt(embed_dim)  # todo: try with input_embed_dim
 
+        # controls the use of stack transformer
+        self.encode_state_machine = args.encode_state_machine
+
+        if self.encode_state_machine:
+            # positions of buffer and stack for each time step
+            self.embed_stack_positions = PositionalEmbedding(
+                args.max_target_positions, args.decoder_embed_dim, 
+                padding_idx, learned=args.decoder_learned_pos,
+            )
+
         self.project_in_dim = Linear(input_embed_dim, embed_dim, bias=False) if embed_dim != input_embed_dim else None
 
         self.embed_positions = PositionalEmbedding(
@@ -375,7 +458,9 @@ class TransformerDecoder(FairseqIncrementalDecoder):
         else:
             self.layer_norm = None
 
-    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **unused):
+    def forward(self, prev_output_tokens, encoder_out, memory, memory_pos,
+                incremental_state=None, logits_mask=None, logits_indices=None,
+                **unused):
         """
         Args:
             prev_output_tokens (LongTensor): previous decoder outputs of shape
@@ -390,11 +475,27 @@ class TransformerDecoder(FairseqIncrementalDecoder):
                 - the decoder's output of shape `(batch, tgt_len, vocab)`
                 - a dictionary with any model-specific outputs
         """
-        x, extra = self.extract_features(prev_output_tokens, encoder_out, incremental_state)
-        x = self.output_layer(x)
+        x, extra = self.extract_features(
+            prev_output_tokens,
+            memory, 
+            memory_pos,
+            encoder_out,
+            incremental_state
+        )
+        x = self.output_layer(
+            x,
+            logits_mask=logits_mask,
+            logits_indices=logits_indices
+        ) 
+
+        # DEBUG: (consumes time)
+        # if (x != x).any():
+        #    import pdb; pdb.set_trace()
+        #    print()
+
         return x, extra
 
-    def extract_features(self, prev_output_tokens, encoder_out=None, incremental_state=None, **unused):
+    def extract_features(self, prev_output_tokens, memory, memory_pos, encoder_out=None, incremental_state=None, **unused):
         """
         Similar to *forward* but only return features.
 
@@ -410,9 +511,12 @@ class TransformerDecoder(FairseqIncrementalDecoder):
         ) if self.embed_positions is not None else None
 
         if incremental_state is not None:
+            # It needs only the last auto-regressive element. Rest is cached.
             prev_output_tokens = prev_output_tokens[:, -1:]
             if positions is not None:
                 positions = positions[:, -1:]
+            memory = memory[:, :, -1:]
+            memory_pos = memory_pos[:, :, -1:]
 
         # embed tokens and positions
         x = self.embed_scale * self.embed_tokens(prev_output_tokens)
@@ -431,13 +535,32 @@ class TransformerDecoder(FairseqIncrementalDecoder):
         inner_states = [x]
 
         # decoder layers
-        for layer in self.layers:
+        for layer_index, layer in enumerate(self.layers):
+
+            # Encode state of state machine as attention masks and encoded
+            # token positions changing for each target action
+            if self.encode_state_machine is None:
+                head_attention_masks = None
+                head_positions = None
+            else:
+                head_attention_masks, head_positions = state_machine_encoder(
+                    self.encode_state_machine,
+                    memory,
+                    memory_pos,
+                    layer.encoder_attn.num_heads,
+                    self.embed_stack_positions,
+                    layer_index,
+                    encoder_out['encoder_padding_mask'] if encoder_out is not None else None
+                )
+
             x, attn = layer(
                 x,
                 encoder_out['encoder_out'] if encoder_out is not None else None,
                 encoder_out['encoder_padding_mask'] if encoder_out is not None else None,
                 incremental_state,
                 self_attn_mask=self.buffered_future_mask(x) if incremental_state is None else None,
+                head_attention_masks=head_attention_masks,
+                head_positions=head_positions
             )
             inner_states.append(x)
 
@@ -452,16 +575,38 @@ class TransformerDecoder(FairseqIncrementalDecoder):
 
         return x, {'attn': attn, 'inner_states': inner_states}
 
-    def output_layer(self, features, **kwargs):
+    def output_layer(self, features, logits_mask=None, logits_indices=None,
+                     **kwargs):
         """Project features to the vocabulary size."""
         if self.adaptive_softmax is None:
             # project back to size of vocabulary
             if self.share_input_output_embed:
-                return F.linear(features, self.embed_tokens.weight)
+                emb_weights = self.embed_tokens.weight
             else:
-                return F.linear(features, self.embed_out)
+                emb_weights = self.embed_out
+            if logits_indices:
+
+                # indices of active logits
+                indices = torch.tensor(list(logits_indices.keys()))
+                # compute only active logits
+                # (batch_size, target_size, target_emb_size)
+                active_output = F.linear(features, emb_weights[indices, :])
+                # forbid masked elements
+                active_output[logits_mask == 0] = float("-Inf")
+                # assign output
+                emb_size = emb_weights.shape[0]
+                batch_size, target_size, _ = features.shape
+                out_shape = (batch_size, target_size, emb_size)
+                output = features.new_ones(out_shape) * float("-Inf")
+                output[:, :, indices] = active_output
+
+            else:    
+                output = F.linear(features, emb_weights)
         else:
-            return features
+            assert not logits_mask
+            output = features
+
+        return output
 
     def max_positions(self):
         """Maximum output length supported by the decoder."""
@@ -605,3 +750,186 @@ def transformer_wmt_en_de_big_t2t(args):
     args.attention_dropout = getattr(args, 'attention_dropout', 0.1)
     args.activation_dropout = getattr(args, 'activation_dropout', 0.1)
     transformer_vaswani_wmt_en_de_big(args)
+
+
+@register_model_architecture('transformer', 'transformer_2x2')
+def transformer_2x2(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', None)
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 2)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 2)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'transformer_6x6')
+def transformer_6x6(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', None)
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 6)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 6)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'transformer_3x8')
+def transformer_3x8(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', None)
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 3)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 8)
+    base_architecture(args)
+
+
+# Stack-Transformer code
+
+
+@register_model_architecture('transformer', 'stack_transformer_2x2_layer0')
+def stack_transformer_2x2_layer0(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "layer0")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 2)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 2)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_6x6_layer0')
+def stack_transformer_6x6_layer0(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "layer0")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 6)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 6)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_2x2_nopos_layer0')
+def stack_transformer_2x2_nopos_layer0(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "layer0_nopos")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 2)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 2)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_2x2_nopos')
+def stack_transformer_2x2_nopos(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "all-layers_nopos")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 2)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 2)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_6x6')
+def stack_transformer_6x6(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "all-layers")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 6)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 6)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_6x6_nopos')
+def stack_transformer_6x6_nopos(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "all-layers_nopos")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 6)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 6)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_6x6_tops_nopos')
+def stack_transformer_6x6_tops_nopos(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "stack_top_nopos")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 6)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 6)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_6x6_only_buffer_nopos')
+def stack_transformer_6x6_only_buffer_nopos(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "only_buffer_nopos")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 6)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 6)
+    base_architecture(args)
+
+
+@register_model_architecture('transformer', 'stack_transformer_6x6_only_stack_nopos')
+def stack_transformer_6x6_only_stack_nopos(args):
+    args.encode_state_machine = getattr(args, 'encode_state_machine', "only_stack_nopos")
+    #
+    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
+    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 512)
+    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 4)
+    args.encoder_layers = getattr(args, 'encoder_layers', 6)
+    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
+    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 512)
+    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)
+    args.decoder_layers = getattr(args, 'decoder_layers', 6)
+    base_architecture(args)
diff --git a/fairseq/modules/multihead_attention.py b/fairseq/modules/multihead_attention.py
index 4da62865..9a1464e8 100644
--- a/fairseq/modules/multihead_attention.py
+++ b/fairseq/modules/multihead_attention.py
@@ -91,7 +91,8 @@ class MultiheadAttention(nn.Module):
             nn.init.xavier_normal_(self.bias_v)
 
     def forward(self, query, key, value, key_padding_mask=None, incremental_state=None,
-                need_weights=True, static_kv=False, attn_mask=None):
+                need_weights=True, static_kv=False, attn_mask=None, 
+                head_attention_masks=None, head_positions=None):
         """Input shape: Time x Batch x Channel
 
         Timesteps can be masked by supplying a T x T mask in the
@@ -137,6 +138,7 @@ class MultiheadAttention(nn.Module):
         else:
             saved_state = None
 
+        # encoder-decoder attention
         if self.self_attention:
             # self-attention
             q, k, v = self.in_proj_qkv(query)
@@ -147,9 +149,34 @@ class MultiheadAttention(nn.Module):
                 assert value is None
                 k = v = None
             else:
+
+                # key/value linear projections 
+                # (source_size, batch_size, target_emb_size) 
+                # -> 
+                # (source_size, batch_size, target_emb_size *2)
+                # -> (chunked) 
+                # (source_size, batch_size, target_emb_size) * 2
                 k = self.in_proj_k(key)
                 v = self.in_proj_v(key)
 
+            if head_positions is not None:
+                # project position embeddings
+                # (batch_size, source_size, target_size, target_emb_size)
+                # ->
+                # (batch_size, source_size, target_size, target_emb_size) * 2
+                head_pos_emb_k = self.in_proj_k(head_positions)
+                head_pos_emb_v = self.in_proj_v(head_positions)
+                # FIXME: this assumes first two heads are stack/buffer 
+                # only the first two heads get stack/buffer pos added
+                head_dim = head_pos_emb_k.shape[3] // self.num_heads
+                head_pos_emb_k[:, :, :, 2 * head_dim:] = 0
+                head_pos_emb_v[:, :, :, 2 * head_dim:] = 0
+
+                # sanity check: assuming first action is a shift, this is
+                # just inserting position zero at leftmost element
+                # assert (head_pos_emb_k[0, 1:, 1, :] == head_pos_emb_k[0, :-1, 0, :]).all()
+                # assert (head_pos_emb_k[0, 0, 1, :] ==  head_pos_emb_k[0, 0, 0, :]).all()
+
         else:
             q = self.in_proj_q(query)
             k = self.in_proj_k(key)
@@ -157,6 +184,9 @@ class MultiheadAttention(nn.Module):
         q *= self.scaling
 
         if self.bias_k is not None:
+            raise NotImplementedError(
+                "attention bias not implemented for stack-trasnformer"
+            )
             assert self.bias_v is not None
             k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])
             v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])
@@ -166,6 +196,22 @@ class MultiheadAttention(nn.Module):
                 key_padding_mask = torch.cat(
                     [key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)
 
+        # NOTE: It splits the emb_dim across heads and unfolds heads in batch
+        # dimension. This is not standard multi-head attention!
+        #
+        # (target_size, batch_size, target_emb_size) 
+        # ->
+        # (target_size, batch_size * num_heads, target_emb_size / num_heads) 
+        # -> 
+        # (batch_size * num_heads, target_size, target_emb_size / num_heads)
+        #
+        # dimension. Heads for same batch element are contiguous on that
+        # dimensions. See example
+        # dummy = torch.zeros(q.shape)   
+        # dummy[:, 0, :] = torch.ones(dummy[:, 0, :].shape)  
+        # dummy2 = dummy.view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)
+        # (dummy2[:self.num_heads, :, :] == 1).sum() == (dummy2 == 1).sum()
+        #
         q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)
         if k is not None:
             k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)
@@ -186,12 +232,22 @@ class MultiheadAttention(nn.Module):
                     v = prev_value
                 else:
                     v = torch.cat((prev_value, v), dim=1)
+
+            # This saves key and value for this head with key e.g.
+            # 'MultiheadAttention.1.attn_state'
             saved_state['prev_key'] = k.view(bsz, self.num_heads, -1, self.head_dim)
             saved_state['prev_value'] = v.view(bsz, self.num_heads, -1, self.head_dim)
 
             self._set_input_buffer(incremental_state, saved_state)
 
         src_len = k.size(1)
+        if head_positions is not None:            
+            # (batch_size, source_size, target_size, target_emb_size) 
+            # -> 
+            # (batch_size * num_heads, source_size, target_size, target_emb_size / num_heads)
+            # assert (head_pos_emb_k[0, 1:, 1, :] == head_pos_emb_k[0, :-1, 0, :]).all()
+            head_pos_emb_k = head_pos_emb_k.transpose(0, 2).contiguous().view(tgt_len, src_len, bsz * self.num_heads, self.head_dim).transpose(0, 2)
+            head_pos_emb_v = head_pos_emb_v.transpose(0, 2).contiguous().view(tgt_len, src_len, bsz * self.num_heads, self.head_dim).transpose(0, 2)
 
         # This is part of a workaround to get around fork/join parallelism
         # not supporting Optional types.
@@ -212,11 +268,36 @@ class MultiheadAttention(nn.Module):
                 key_padding_mask = torch.cat(
                     [key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)], dim=1)
 
+        # Compute unnormalized attention
+        # q              (batch_size * num_heads, target_size, target_emb_size / num_heads)
+        # k              (batch_size * num_heads, source_size, target_emb_size / num_heads)
+        # ->
+        # attn_weights   (batch_size * num_heads, source_size, target_size)
         attn_weights = torch.bmm(q, k.transpose(1, 2))
+
+        if head_positions is not None:
+            # if buffer/stack positions provided, add them to attention computation
+            # Note that batched inner product is here implemented as
+            # elements-wise product and sum across common axis
+            # head_positions (batch_size * num_heads, source_size, target_size, target_emb_size / num_heads) 
+            # q              (batch_size * num_heads, target_size, target_emb_size / num_heads)
+            # ->
+            # attn_weights   (batch_size * num_heads, source_size, target_size)
+            attn_weights += self.scaling * (q.unsqueeze(1) * head_pos_emb_k).sum(3).transpose(1, 2)
+
+        # FIXME: What is this
         attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)
 
         assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
 
+        # Stack/Buffer individual mask per head
+        if head_attention_masks is not None:
+            if self.onnx_trace:
+              # dunno whats this better die
+              raise NotImplementedError()
+            # mask in log domain with pre_mask
+            attn_weights += head_attention_masks[0]
+
         if attn_mask is not None:
             attn_mask = attn_mask.unsqueeze(0)
             if self.onnx_trace:
@@ -244,7 +325,27 @@ class MultiheadAttention(nn.Module):
         ).type_as(attn_weights)
         attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)
 
+        # post mask for empty buffer/stack
+        if head_attention_masks is not None:
+            # sanity check, this really blocks only all inf weights
+            attn_weights = attn_weights * head_attention_masks[1]
+
+        # Compute attended source
+        # attn_weights   (batch_size * num_heads, target_size, source_size)
+        # v              (batch_size * num_heads, source_size, target_emb_size / num_heads)
+        # ->
+        # attn           (batch_size * num_heads, target_size, target_emb_size / num_heads)
         attn = torch.bmm(attn_weights, v)
+        if head_positions is not None:
+            # if buffer/stack positions provided, add them to attention computation
+            # Note that batched inner product is here implemented as
+            # elements-wise product and sum across common axis
+            # attn_weights   (batch_size * num_heads, target_size, source_size)
+            # head_positions (batch_size * num_heads, source_size, target_size, target_emb_size / num_heads) 
+            # ->
+            # attn           (batch_size * num_heads, target_size, target_emb_size / num_heads)
+            attn += (attn_weights.transpose(1, 2).unsqueeze(3) * head_pos_emb_v).sum(1)
+
         assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
         if (self.onnx_trace and attn.size(1) == 1):
             # when ONNX tracing a single decoder step (sequence length == 1)
diff --git a/fairseq/modules/transformer_layer.py b/fairseq/modules/transformer_layer.py
index 5da4909c..fd747768 100644
--- a/fairseq/modules/transformer_layer.py
+++ b/fairseq/modules/transformer_layer.py
@@ -193,6 +193,8 @@ class TransformerDecoderLayer(nn.Module):
         prev_attn_state=None,
         self_attn_mask=None,
         self_attn_padding_mask=None,
+        head_attention_masks=None, 
+        head_positions=None
     ):
         """
         Args:
@@ -228,6 +230,7 @@ class TransformerDecoderLayer(nn.Module):
             residual = x
             x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, before=True)
             if prev_attn_state is not None:
+                # Load key and value from a previous layer if existing
                 if incremental_state is None:
                     incremental_state = {}
                 prev_key, prev_value = prev_attn_state
@@ -241,6 +244,8 @@ class TransformerDecoderLayer(nn.Module):
                 incremental_state=incremental_state,
                 static_kv=True,
                 need_weights=(not self.training and self.need_attn),
+                head_attention_masks=head_attention_masks,
+                head_positions=head_positions
             )
             x = F.dropout(x, p=self.dropout, training=self.training)
             x = residual + x
diff --git a/fairseq/optim/adam.py b/fairseq/optim/adam.py
index 0df11820..bda396b6 100644
--- a/fairseq/optim/adam.py
+++ b/fairseq/optim/adam.py
@@ -48,7 +48,8 @@ class FairseqAdam(FairseqOptimizer):
         """
         return {
             'lr': self.args.lr[0],
-            'betas': eval(self.args.adam_betas),
+            # extra scaping for command line literal artifacts
+            'betas': eval(self.args.adam_betas.replace('\'', '')),
             'eps': self.args.adam_eps,
             'weight_decay': self.args.weight_decay,
         }
diff --git a/fairseq/options.py b/fairseq/options.py
index b02a5778..da5fc404 100644
--- a/fairseq/options.py
+++ b/fairseq/options.py
@@ -243,6 +243,20 @@ def add_preprocess_args(parser):
                        help="Pad dictionary size to be multiple of N")
     group.add_argument("--workers", metavar="N", default=1, type=int,
                        help="number of parallel workers")
+
+    # for pretrained external embeddings
+    group.add_argument("--pretrained-embed", default='roberta.base',
+                       help="Type of pretrained embedding")
+    # NOTE: Previous default "17 18 19 20 21 22 23 24"
+    group.add_argument('--bert-layers', nargs='+', type=int,
+                       help='RoBERTa layers to extract (default last)')
+    # wether to separate by space or tab
+    group.add_argument("--tokenize-by-whitespace", action='store_true',
+                       help="Tokenize by whitespace or tab")
+
+    # for stack-transformer
+    add_state_machine_args(group)
+
     # fmt: on
     return parser
 
@@ -351,6 +365,9 @@ def add_optimization_args(parser):
                        help='stop training when the learning rate reaches this minimum')
     group.add_argument('--use-bmuf', default=False, action='store_true',
                        help='specify global optimizer for syncing models on different GPUs/shards')
+
+
+    group.add_argument('--burnthrough', '--bt', default=0, type=int, help='Do not evaluate nor save until this epoch')
     # fmt: on
     return group
 
@@ -398,6 +415,9 @@ def add_checkpoint_args(parser):
 
 
 def add_common_eval_args(group):
+
+    add_state_machine_args(group)
+
     # fmt: off
     group.add_argument('--path', metavar='FILE',
                        help='path(s) to model file(s), colon separated')
@@ -413,6 +433,22 @@ def add_common_eval_args(group):
     # fmt: on
 
 
+def add_state_machine_args(group):
+    group.add_argument('--machine-type', type=str, default=None,
+                       choices=['AMR', 'NER', 'NER2','WSD','MCR' ,'SRL', 'dep-parsing'],
+                       help='Type of state machine used in decoding')
+    group.add_argument('--machine-rules', type=str, default=None,
+                       help='json files with extra rules for the state machine')
+    group.add_argument('--entity-rules', type=str, default=None,
+                       help='json files with entity rules for the state machine')
+    group.add_argument('--gold-annotations', type=str, nargs='+',
+                       help='Path to gold annotations file used for reward calculation')
+    group.add_argument('--gold-episode-ratio', type=float,
+                       help='Ratio of episodes in batch set to follow gold')
+    group.add_argument('--batch-normalize-reward', action='store_true',
+                       help='Normalize rewards in batch')
+
+
 def add_eval_lm_args(parser):
     group = parser.add_argument_group('LM Evaluation')
     add_common_eval_args(group)
@@ -482,6 +518,8 @@ def add_generation_args(parser):
                        help='strength of diversity penalty for Diverse Beam Search')
     group.add_argument('--print-alignment', action='store_true',
                        help='if set, uses attention feedback to compute and print alignment to source tokens')
+    group.add_argument("--tokenize-by-whitespace", action='store_true',
+                       help="Tokenize by whitespace or tab")
     # fmt: on
     return group
 
diff --git a/fairseq/sequence_generator.py b/fairseq/sequence_generator.py
index 3b100b96..756761b5 100644
--- a/fairseq/sequence_generator.py
+++ b/fairseq/sequence_generator.py
@@ -9,6 +9,9 @@ import torch
 
 from fairseq import search
 from fairseq.models import FairseqIncrementalDecoder
+from transition_amr_parser.stack_transformer.amr_state_machine import (
+    fix_shift_multi_task, update_machine
+)
 
 
 class SequenceGenerator(object):
@@ -31,6 +34,7 @@ class SequenceGenerator(object):
         diverse_beam_strength=0.5,
         match_source_len=False,
         no_repeat_ngram_size=0,
+        state_machine=None
     ):
         """Generates translations of a given source sentence.
 
@@ -81,6 +85,8 @@ class SequenceGenerator(object):
         self.temperature = temperature
         self.match_source_len = match_source_len
         self.no_repeat_ngram_size = no_repeat_ngram_size
+        # Type of state machine used
+        self.state_machine = state_machine
 
         assert sampling_topk < 0 or sampling, '--sampling-topk requires --sampling'
         assert sampling_topp < 0 or sampling, '--sampling-topp requires --sampling'
@@ -125,6 +131,10 @@ class SequenceGenerator(object):
             if k != 'prev_output_tokens'
         }
 
+        # Add dummy memory and memory pos
+        encoder_input['memory'] = None
+        encoder_input['memory_pos'] = None
+
         src_tokens = encoder_input['src_tokens']
         src_lengths = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)
         input_size = src_tokens.size()
@@ -136,11 +146,8 @@ class SequenceGenerator(object):
         if self.match_source_len:
             max_len = src_lengths.max().item()
         else:
-            max_len = min(
-                int(self.max_len_a * src_len + self.max_len_b),
-                # exclude the EOS marker
-                model.max_decoder_positions() - 1,
-            )
+            # FIXME: Hard coded limit for transition-based parsing
+            max_len = int(math.ceil(src_lengths.max().item() * 10))
 
         # compute the encoder output for each beam
         encoder_outs = model.forward_encoder(encoder_input)
@@ -276,6 +283,14 @@ class SequenceGenerator(object):
                     newly_finished.append(unfin_idx)
             return newly_finished
 
+        # Initialize state machine and get first states
+        # get rules from model folder
+        self.state_machine.reset(
+            src_tokens[new_order, :].clone().detach(),
+            src_lengths[new_order].clone().detach(),
+            tokens.shape[1]
+        ) 
+
         reorder_state = None
         batch_idxs = None
         for step in range(max_len + 1):  # one extra step for EOS marker
@@ -288,17 +303,47 @@ class SequenceGenerator(object):
                 model.reorder_incremental_state(reorder_state)
                 encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)
 
+                # reorder state machine
+                self.state_machine.reoder_machine(reorder_state)
+
+                # Update state machine
+                update_machine(step - 1, tokens, scores, self.state_machine) 
+
+            # get active logit indices for this time-step for the entire batch
+            # and masks for each sentence. Gather all variables defining the
+            # parser state
+            logits_indices, logits_mask = self.state_machine.get_active_logits()
+            parser_state = (
+                self.state_machine.memory[:, :, :step + 1].clone(),
+                self.state_machine.memory_pos[:, :, :step + 1].clone(),
+                logits_mask,
+                logits_indices
+            )
+
+            # call model with pre-computed encoder, previous generated actions
+            # tokens                      (bsz * beam_size, max_len + 2)       long
+            # encoder_out                 (src_len, bsz * beam_size, emb_dim)  float
+            # self.state_machine.memory        (bsz * beam_size, src_len, max_len)  
+            # self.state_machine.memory_pos    (bsz * beam_size, src_len, max_len)
+            # (tokens) and state machine status
             lprobs, avg_attn_scores = model.forward_decoder(
-                tokens[:, :step + 1], encoder_outs, temperature=self.temperature,
+                tokens[:, :step + 1], encoder_outs, parser_state, temperature=self.temperature
             )
 
-            lprobs[:, self.pad] = -math.inf  # never select pad
-            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty
+            # Fix lprobs for labeled SHIFT
+            lprobs = fix_shift_multi_task(
+                lprobs,
+                self.state_machine,
+                self.state_machine.tgt_dict, 
+                logits_indices
+            )
 
             # handle min and max length constraints
             if step >= max_len:
                 lprobs[:, :self.eos] = -math.inf
                 lprobs[:, self.eos + 1:] = -math.inf
+                # FIXME: Added this to avoid no option, why dont they do this?
+                lprobs[:, self.eos] = 0.0
             elif step < self.min_len:
                 lprobs[:, self.eos] = -math.inf
 
@@ -370,6 +415,12 @@ class SequenceGenerator(object):
                 for bbsz_idx in range(bsz * beam_size):
                     lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf
 
+            # Beam search
+            # lprobs  (batch_size, in_beam_size, vocab_size)
+            # ->
+            # cand_scores  (batch_size, output_beam_size)
+            # cand_indices (batch_size, output_beam_size)
+            # cand_beams   (batch_size, output_beam_size)
             cand_scores, cand_indices, cand_beams = self.search.step(
                 step,
                 lprobs.view(bsz, -1, self.vocab_size),
@@ -539,12 +590,13 @@ class EnsembleModel(torch.nn.Module):
         return [model.encoder(**encoder_input) for model in self.models]
 
     @torch.no_grad()
-    def forward_decoder(self, tokens, encoder_outs, temperature=1.):
+    def forward_decoder(self, tokens, encoder_outs, parser_state, temperature=1):
         if len(self.models) == 1:
             return self._decode_one(
                 tokens,
                 self.models[0],
                 encoder_outs[0] if self.has_encoder() else None,
+                parser_state,
                 self.incremental_states,
                 log_probs=True,
                 temperature=temperature,
@@ -557,9 +609,10 @@ class EnsembleModel(torch.nn.Module):
                 tokens,
                 model,
                 encoder_out,
+                parser_state,
                 self.incremental_states,
                 log_probs=True,
-                temperature=temperature,
+                temperature=temperature
             )
             log_probs.append(probs)
             if attn is not None:
@@ -573,12 +626,25 @@ class EnsembleModel(torch.nn.Module):
         return avg_probs, avg_attn
 
     def _decode_one(
-        self, tokens, model, encoder_out, incremental_states, log_probs,
-        temperature=1.,
+        self, tokens, model, encoder_out, parser_state, incremental_states, 
+        log_probs, temperature=1.,
     ):
         if self.incremental_states is not None:
-            decoder_out = list(model.decoder(tokens, encoder_out, incremental_state=self.incremental_states[model]))
+
+            # unpack state
+            memory, memory_pos, logits_mask, logits_indices = parser_state 
+
+            decoder_out = list(model.decoder(
+                tokens,
+                encoder_out,
+                memory,
+                memory_pos,
+                incremental_state=self.incremental_states[model],
+                logits_mask=logits_mask,
+                logits_indices=logits_indices
+            ))
         else:
+            raise NotImplementedError()
             decoder_out = list(model.decoder(tokens, encoder_out))
         decoder_out[0] = decoder_out[0][:, -1:, :]
         if temperature != 1.:
diff --git a/fairseq/tasks/fairseq_task.py b/fairseq/tasks/fairseq_task.py
index 3dea0716..5a7bfde7 100644
--- a/fairseq/tasks/fairseq_task.py
+++ b/fairseq/tasks/fairseq_task.py
@@ -4,9 +4,11 @@
 # LICENSE file in the root directory of this source tree.
 
 import torch
+import numpy as np
 
 from fairseq import tokenizer
 from fairseq.data import data_utils, FairseqDataset, iterators, Dictionary
+from transition_amr_parser.stack_transformer.amr_state_machine import StateMachineBatch
 
 
 class FairseqTask(object):
@@ -34,7 +36,7 @@ class FairseqTask(object):
         return Dictionary.load(filename)
 
     @classmethod
-    def build_dictionary(cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8):
+    def build_dictionary(cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8, tokenize=tokenizer.tokenize_line):
         """Build the dictionary
 
         Args:
@@ -49,7 +51,7 @@ class FairseqTask(object):
         """
         d = Dictionary()
         for filename in filenames:
-            Dictionary.add_file_to_dictionary(filename, d, tokenizer.tokenize_line, workers)
+            Dictionary.add_file_to_dictionary(filename, d, tokenize, workers)
         d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)
         return d
 
@@ -91,6 +93,8 @@ class FairseqTask(object):
         self, dataset, max_tokens=None, max_sentences=None, max_positions=None,
         ignore_invalid_inputs=False, required_batch_size_multiple=1,
         seed=1, num_shards=1, shard_id=0, num_workers=0, epoch=0,
+        large_sent_first=False, whitelisted_indices=None, 
+        blacklisted_indices=None
     ):
         """
         Get an iterator that yields batches of data from the given dataset.
@@ -128,6 +132,18 @@ class FairseqTask(object):
         # get indices ordered by example size
         with data_utils.numpy_seed(seed):
             indices = dataset.ordered_indices()
+            # invert order to start by bigger ones
+            if large_sent_first:
+                indices = indices[::-1]
+
+        # set whitelisted indices
+        if whitelisted_indices is not None:
+            indices = np.array(whitelisted_indices)
+
+        # filter blacklisted indices
+        if blacklisted_indices is not None:
+            blacklisted_indices = np.array(blacklisted_indices)
+            indices = np.setdiff1d(indices, blacklisted_indices)
 
         # filter examples that are too large
         if max_positions is not None:
@@ -204,6 +220,14 @@ class FairseqTask(object):
                 diverse_beam_strength=getattr(args, 'diverse_beam_strength', 0.5),
                 match_source_len=getattr(args, 'match_source_len', False),
                 no_repeat_ngram_size=getattr(args, 'no_repeat_ngram_size', 0),
+                # State machine for parsing
+                state_machine=StateMachineBatch(
+                    self.source_dictionary,
+                    self.target_dictionary,
+                    args.machine_type,
+                    machine_rules=args.machine_rules,
+                    entity_rules=args.entity_rules
+                )
             )
 
     def train_step(self, sample, model, criterion, optimizer, ignore_grad=False):
@@ -228,9 +252,24 @@ class FairseqTask(object):
         """
         model.train()
         loss, sample_size, logging_output = criterion(model, sample)
+#        if torch.isnan(loss):
+#            import ipdb; ipdb.set_trace(context=30)
+#            loss, sample_size, logging_output = criterion(model, sample)
+            
         if ignore_grad:
             loss *= 0
         optimizer.backward(loss)
+
+#        # NaN weigths
+#        nan_weights = {
+#            name: param  
+#            for name, param in model.named_parameters() 
+#            if torch.isnan(param).any()
+#        }
+#        if nan_weights:
+#            import ipdb; ipdb.set_trace(context=30)
+#            print()
+        
         return loss, sample_size, logging_output
 
     def valid_step(self, sample, model, criterion):
diff --git a/fairseq/tasks/translation.py b/fairseq/tasks/translation.py
index d3f51cb3..16280a3c 100644
--- a/fairseq/tasks/translation.py
+++ b/fairseq/tasks/translation.py
@@ -22,14 +22,23 @@ def load_langpair_dataset(
     src, src_dict,
     tgt, tgt_dict,
     combine, dataset_impl, upsample_primary,
-    left_pad_source, left_pad_target, max_source_positions, max_target_positions,
+    left_pad_source, left_pad_target, max_source_positions, max_target_positions, 
+    state_machine=True
 ):
     def split_exists(split, src, tgt, lang, data_path):
         filename = os.path.join(data_path, '{}.{}-{}.{}'.format(split, src, tgt, lang))
         return indexed_dataset.dataset_exists(filename, impl=dataset_impl)
 
     src_datasets = []
+    src_fixed_embeddings = []
+    src_wordpieces = []
+    src_wp2w = []
+
     tgt_datasets = []
+    memory_datasets = []
+    memory_pos_datasets = []
+    target_mask_datasets = []
+    active_logits_datasets = []
 
     for k in itertools.count():
         split_k = split + (str(k) if k > 0 else '')
@@ -44,14 +53,43 @@ def load_langpair_dataset(
                 break
             else:
                 raise FileNotFoundError('Dataset not found: {} ({})'.format(split, data_path))
-
-        src_datasets.append(
-            data_utils.load_indexed_dataset(prefix + src, src_dict, dataset_impl)
-        )
-        tgt_datasets.append(
-            data_utils.load_indexed_dataset(prefix + tgt, tgt_dict, dataset_impl)
+        
+        # source
+        src_file = (prefix + src, src_dict, dataset_impl)
+        src_datasets.append(data_utils.load_indexed_dataset(*src_file))
+
+        # pre-trained embeddings
+        fixed_embeddings_file = (prefix + 'en.bert', None, dataset_impl)
+        src_fixed_embeddings.append(
+            data_utils.load_indexed_dataset(*fixed_embeddings_file)
         )
 
+        # wordpieces
+        wordpieces_file = (prefix + 'en.wordpieces', None, dataset_impl)
+        src_wordpieces.append(data_utils.load_indexed_dataset(*wordpieces_file))
+
+        # wordpieces to word map
+        wp2w_file = (prefix + 'en.wp2w', None, dataset_impl)
+        src_wp2w.append(data_utils.load_indexed_dataset(*wp2w_file))
+
+        # actions
+        tgt_file = prefix + tgt, tgt_dict, dataset_impl
+        tgt_datasets.append(data_utils.load_indexed_dataset(*tgt_file))
+
+        # state machine states (buffer/stack) and positions
+        memory_file = prefix + 'memory', None, dataset_impl
+        memory_datasets.append(data_utils.load_indexed_dataset(*memory_file))
+        memory_pos_file = prefix + 'memory_pos', None, dataset_impl
+        memory_pos_datasets.append(data_utils.load_indexed_dataset(*memory_pos_file))
+
+        # logit masks
+        target_mask_file = prefix + 'target_masks', None, dataset_impl
+        target_mask_datasets.append(data_utils.load_indexed_dataset(*target_mask_file))
+
+        # active logits
+        active_logits_file = prefix + 'active_logits', None, dataset_impl
+        active_logits_datasets.append(data_utils.load_indexed_dataset(*active_logits_file))
+
         print('| {} {} {}-{} {} examples'.format(data_path, split_k, src, tgt, len(src_datasets[-1])))
 
         if not combine:
@@ -61,7 +99,16 @@ def load_langpair_dataset(
 
     if len(src_datasets) == 1:
         src_dataset, tgt_dataset = src_datasets[0], tgt_datasets[0]
+        src_fixed_embeddings = src_fixed_embeddings[0]
+        src_wordpieces = src_wordpieces[0]
+        src_wp2w = src_wp2w[0]
+        memory_dataset = memory_datasets[0]
+        memory_pos_dataset = memory_pos_datasets[0]
+        target_mask_datasets = target_mask_datasets[0]
+        active_logits_datasets = active_logits_datasets[0]
     else:
+        # not implemented for stack-transformer
+        raise NotImplementedError()
         sample_ratios = [1] * len(src_datasets)
         sample_ratios[0] = upsample_primary
         src_dataset = ConcatDataset(src_datasets, sample_ratios)
@@ -69,11 +116,19 @@ def load_langpair_dataset(
 
     return LanguagePairDataset(
         src_dataset, src_dataset.sizes, src_dict,
+        src_fixed_embeddings, src_fixed_embeddings.sizes,
+        src_wordpieces, src_wordpieces.sizes,
+        src_wp2w, src_wp2w.sizes,
         tgt_dataset, tgt_dataset.sizes, tgt_dict,
+        memory_dataset, memory_dataset.sizes,
+        memory_pos_dataset, memory_pos_dataset.sizes,
+        target_mask_datasets, target_mask_datasets.sizes,
+        active_logits_datasets, active_logits_datasets.sizes,
         left_pad_source=left_pad_source,
         left_pad_target=left_pad_target,
         max_source_positions=max_source_positions,
         max_target_positions=max_target_positions,
+        state_machine=state_machine
     )
 
 
@@ -165,7 +220,7 @@ class TranslationTask(FairseqTask):
 
         return cls(args, src_dict, tgt_dict)
 
-    def load_dataset(self, split, epoch=0, combine=False, **kwargs):
+    def load_dataset(self, split, epoch=0, combine=False, state_machine=True, **kwargs):
         """Load a given dataset split.
 
         Args:
@@ -186,6 +241,7 @@ class TranslationTask(FairseqTask):
             left_pad_target=self.args.left_pad_target,
             max_source_positions=self.args.max_source_positions,
             max_target_positions=self.args.max_target_positions,
+            state_machine=state_machine
         )
 
     def build_dataset_for_inference(self, src_tokens, src_lengths):
diff --git a/fairseq/tokenizer.py b/fairseq/tokenizer.py
index 8c4d694a..0c25a6f6 100644
--- a/fairseq/tokenizer.py
+++ b/fairseq/tokenizer.py
@@ -12,3 +12,8 @@ def tokenize_line(line):
     line = SPACE_NORMALIZER.sub(" ", line)
     line = line.strip()
     return line.split()
+
+
+def tab_tokenize(line):
+    line = line.strip()
+    return line.split('\t')
diff --git a/fairseq/utils.py b/fairseq/utils.py
index 1b664cbf..16fa44f2 100644
--- a/fairseq/utils.py
+++ b/fairseq/utils.py
@@ -18,6 +18,8 @@ import torch.nn.functional as F
 
 from fairseq.modules import gelu, gelu_accurate
 
+from fairseq.tokenizer import tokenize_line
+
 
 def load_ensemble_for_inference(filenames, task, model_arg_overrides=None):
     from fairseq import checkpoint_utils
@@ -140,10 +142,9 @@ def load_embedding(embed_dict, vocab, embedding):
     return embedding
 
 
-def replace_unk(hypo_str, src_str, alignment, align_dict, unk):
-    from fairseq import tokenizer
+def replace_unk(hypo_str, src_str, alignment, align_dict, unk, line_tokenizer=tokenize_line):
     # Tokens are strings here
-    hypo_tokens = tokenizer.tokenize_line(hypo_str)
+    hypo_tokens = tokenize_line(hypo_str)
     # TODO: Very rare cases where the replacement is '<eos>' should be handled gracefully
     src_tokens = tokenizer.tokenize_line(src_str) + ['<eos>']
     for i, ht in enumerate(hypo_tokens):
@@ -154,14 +155,15 @@ def replace_unk(hypo_str, src_str, alignment, align_dict, unk):
     return ' '.join(hypo_tokens)
 
 
-def post_process_prediction(hypo_tokens, src_str, alignment, align_dict, tgt_dict, remove_bpe=None):
-    hypo_str = tgt_dict.string(hypo_tokens, remove_bpe)
+def post_process_prediction(hypo_tokens, src_str, alignment, align_dict, tgt_dict, remove_bpe=None, split_token=' ', line_tokenizer=tokenize_line):
+
+    hypo_str = tgt_dict.string(hypo_tokens, remove_bpe, split_token=split_token)
     if align_dict is not None:
         hypo_str = replace_unk(hypo_str, src_str, alignment, align_dict, tgt_dict.unk_string())
     if align_dict is not None or remove_bpe is not None:
         # Convert back to tokens for evaluating with unk replacement or without BPE
         # Note that the dictionary can be modified inside the method.
-        hypo_tokens = tgt_dict.encode_line(hypo_str, add_if_not_exist=True)
+        hypo_tokens = tgt_dict.encode_line(hypo_str, add_if_not_exist=True, line_tokenizer=tokenize_line)
     return hypo_tokens, hypo_str, alignment
 
 
diff --git a/generate.py b/generate.py
index c23cc798..7c69f65b 100644
--- a/generate.py
+++ b/generate.py
@@ -11,6 +11,9 @@ import torch
 
 from fairseq import bleu, checkpoint_utils, options, progress_bar, tasks, utils
 from fairseq.meters import StopwatchMeter, TimeMeter
+from fairseq.tokenizer import tab_tokenize
+
+from transition_amr_parser.stack_transformer.data_utils import Examples
 
 
 def main(args):
@@ -30,7 +33,9 @@ def main(args):
 
     # Load dataset splits
     task = tasks.setup_task(args)
-    task.load_dataset(args.gen_subset)
+    # Note: states are not needed since they will be provided by the state
+    # machine
+    task.load_dataset(args.gen_subset, state_machine=False)
 
     # Set dictionaries
     try:
@@ -59,7 +64,8 @@ def main(args):
             model.cuda()
 
     # Load alignment dictionary for unknown word replacement
-    # (None if no unknown word replacement, empty if no path to align dictionary)
+    # (None if no unknown word replacement, empty if no path to align
+    # dictionary)
     align_dict = utils.load_align_dict(args.replace_unk)
 
     # Load dataset (possibly sharded)
@@ -67,15 +73,13 @@ def main(args):
         dataset=task.dataset(args.gen_subset),
         max_tokens=args.max_tokens,
         max_sentences=args.max_sentences,
-        max_positions=utils.resolve_max_positions(
-            task.max_positions(),
-            *[model.max_positions() for model in models]
-        ),
+        max_positions=None,
         ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,
         required_batch_size_multiple=args.required_batch_size_multiple,
         num_shards=args.num_shards,
         shard_id=args.shard_id,
         num_workers=args.num_workers,
+        large_sent_first=False
     ).next_epoch_itr(shuffle=False)
 
     # Initialize generator
@@ -89,11 +93,15 @@ def main(args):
         scorer = bleu.Scorer(tgt_dict.pad(), tgt_dict.eos(), tgt_dict.unk())
     num_sentences = 0
     has_target = True
+
+    examples = Examples(args.path, args.results_path, args.gen_subset, args.nbest)
+
     with progress_bar.build_progress_bar(args, itr) as t:
         wps_meter = TimeMeter()
         for sample in t:
             sample = utils.move_to_cuda(sample) if use_cuda else sample
             if 'net_input' not in sample:
+                raise Exception("Did not expect empty sample")
                 continue
 
             prefix_tokens = None
@@ -141,10 +149,21 @@ def main(args):
                         align_dict=align_dict,
                         tgt_dict=tgt_dict,
                         remove_bpe=args.remove_bpe,
+                        # FIXME: AMR specific
+                        split_token="\t",
+                        line_tokenizer=tab_tokenize,
                     )
 
+                    # update the list of examples
+                    examples.append({
+                        'hypothesis': hypo_str,
+                        'reference': target_str,
+                        'src_str': src_str,
+                        'sample_id': sample_id
+                    })
+
                     if not args.quiet:
-                        print('H-{}\t{}\t{}'.format(sample_id, hypo['score'], hypo_str))
+                        print('H-{}\t{}\t{}'.format(sample_id, hypo_str, hypo['score']))
                         print('P-{}\t{}'.format(
                             sample_id,
                             ' '.join(map(
@@ -173,6 +192,9 @@ def main(args):
             t.log({'wps': round(wps_meter.avg)})
             num_sentences += sample['nsentences']
 
+    # Save examples to files
+    examples.save()
+
     print('| Translated {} sentences ({} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)'.format(
         num_sentences, gen_timer.n, gen_timer.sum, num_sentences / gen_timer.sum, 1. / gen_timer.avg))
     if has_target:
diff --git a/preprocess.py b/preprocess.py
index a157feeb..5c22448a 100644
--- a/preprocess.py
+++ b/preprocess.py
@@ -18,6 +18,8 @@ from multiprocessing import Pool
 import os
 import shutil
 
+from fairseq.tokenizer import tokenize_line, tab_tokenize
+from transition_amr_parser.stack_transformer.preprocess import make_state_machine
 
 def main(args):
     utils.import_user_module(args)
@@ -29,9 +31,20 @@ def main(args):
 
     task = tasks.get_task(args.task)
 
+    # Determine tokenizer
+    if args.tokenize_by_whitespace:
+        # for the rest normal tokenization
+        tokenize = tokenize_line
+    else:
+        # for AMR we expect tokenization by tab
+        tokenize = tab_tokenize
+
     def train_path(lang):
         return "{}{}".format(args.trainpref, ("." + lang) if lang else "")
 
+    def valid_path(lang):
+        return "{}{}".format(args.validpref, ("." + lang) if lang else "")
+
     def file_name(prefix, lang):
         fname = prefix
         if lang is not None:
@@ -44,7 +57,7 @@ def main(args):
     def dict_path(lang):
         return dest_path("dict", lang) + ".txt"
 
-    def build_dictionary(filenames, src=False, tgt=False):
+    def build_dictionary(filenames, src=False, tgt=False, tokenize=tokenize):
         assert src ^ tgt
         return task.build_dictionary(
             filenames,
@@ -52,6 +65,7 @@ def main(args):
             threshold=args.thresholdsrc if src else args.thresholdtgt,
             nwords=args.nwordssrc if src else args.nwordstgt,
             padding_factor=args.padding_factor,
+            tokenize=tokenize
         )
 
     if not args.srcdict and os.path.exists(dict_path(args.source_lang)):
@@ -70,7 +84,7 @@ def main(args):
         else:
             assert args.trainpref, "--trainpref must be set if --srcdict is not specified"
             src_dict = build_dictionary(
-                {train_path(lang) for lang in [args.source_lang, args.target_lang]}, src=True
+                {train_path(lang) for lang in [args.source_lang, args.target_lang]}, src=True, tokenize=tokenize
             )
         tgt_dict = src_dict
     else:
@@ -78,14 +92,20 @@ def main(args):
             src_dict = task.load_dictionary(args.srcdict)
         else:
             assert args.trainpref, "--trainpref must be set if --srcdict is not specified"
-            src_dict = build_dictionary([train_path(args.source_lang)], src=True)
+            # FIXME: Hacky way to avoid <unks>, add dev
+            # As long as we use only BERT for input this should be fine
+            dict_paths = [train_path(args.source_lang), valid_path(args.source_lang)]
+            src_dict = build_dictionary(dict_paths, src=True, tokenize=tokenize)
 
         if target:
             if args.tgtdict:
                 tgt_dict = task.load_dictionary(args.tgtdict)
             else:
                 assert args.trainpref, "--trainpref must be set if --tgtdict is not specified"
-                tgt_dict = build_dictionary([train_path(args.target_lang)], tgt=True)
+                # Hacky way to avoid <unks>, add dev
+                # dict_paths = [train_path(args.target_lang), valid_path(args.target_lang)]
+                dict_paths = [train_path(args.target_lang)]
+                tgt_dict = build_dictionary(dict_paths, tgt=True, tokenize=tokenize)
         else:
             tgt_dict = None
 
@@ -109,6 +129,7 @@ def main(args):
         offsets = Binarizer.find_offsets(input_file, num_workers)
         pool = None
         if num_workers > 1:
+            raise Exception("to set tokenize, we can not allow num_workers > 1")
             pool = Pool(processes=num_workers - 1)
             for worker_id in range(1, num_workers):
                 prefix = "{}{}".format(output_prefix, worker_id)
@@ -121,7 +142,8 @@ def main(args):
                         prefix,
                         lang,
                         offsets[worker_id],
-                        offsets[worker_id + 1]
+                        offsets[worker_id + 1],
+                        tokenize # this may fail
                     ),
                     callback=merge_result
                 )
@@ -132,7 +154,9 @@ def main(args):
         merge_result(
             Binarizer.binarize(
                 input_file, vocab, lambda t: ds.add_item(t),
-                offset=0, end=offsets[1]
+                offset=0, end=offsets[1],
+                append_eos=False,
+                tokenize=tokenize
             )
         )
         if num_workers > 1:
@@ -184,6 +208,9 @@ def main(args):
     if target:
         make_all(args.target_lang, tgt_dict)
 
+    # Make preprocessing data for the state machine
+    make_state_machine(args, src_dict, tgt_dict, tokenize=tokenize)
+
     print("| Wrote preprocessed data to {}".format(args.destdir))
 
     if args.alignfile:
@@ -229,7 +256,10 @@ def main(args):
                 print("{} {}".format(src_dict[k], tgt_dict[v]), file=f)
 
 
-def binarize(args, filename, vocab, output_prefix, lang, offset, end, append_eos=True):
+# FIXME: This was False pre BERT merge
+# def binarize(args, filename, vocab, output_prefix, lang, offset, end, append_eos=False):
+
+def binarize(args, filename, vocab, output_prefix, lang, offset, end, append_eos=True, tokenize=tokenize_line):
     ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, "bin"),
                                       impl=args.dataset_impl, vocab_size=len(vocab))
 
@@ -237,7 +267,7 @@ def binarize(args, filename, vocab, output_prefix, lang, offset, end, append_eos
         ds.add_item(tensor)
 
     res = Binarizer.binarize(filename, vocab, consumer, append_eos=append_eos,
-                             offset=offset, end=end)
+                             offset=offset, end=end, tokenize=tokenize)
     ds.finalize(dataset_dest_file(args, output_prefix, lang, "idx"))
     return res
 
diff --git a/train.py b/train.py
index b73e362d..80a271e9 100644
--- a/train.py
+++ b/train.py
@@ -79,7 +79,11 @@ def main(args, init_distributed=False):
         # train for one epoch
         train(args, trainer, task, epoch_itr)
 
-        if not args.disable_validation and epoch_itr.epoch % args.validate_interval == 0:
+        if (
+            not args.disable_validation 
+            and epoch_itr.epoch % args.validate_interval == 0
+            and epoch_itr.epoch > args.burnthrough
+        ):
             valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
         else:
             valid_losses = [None]
@@ -88,7 +92,10 @@ def main(args, init_distributed=False):
         lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])
 
         # save checkpoint
-        if epoch_itr.epoch % args.save_interval == 0:
+        if (
+            epoch_itr.epoch % args.save_interval == 0 
+            and epoch_itr.epoch > args.burnthrough
+        ):
             checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])
 
         if ':' in getattr(args, 'data', ''):
@@ -114,10 +121,20 @@ def train(args, trainer, task, epoch_itr):
         args, itr, epoch_itr.epoch, no_progress_bar='simple',
     )
 
+    #from fairseq.debug_tools import add_hook_to_model, break_on_nan
+    #add_hook_to_model(trainer.model, break_on_nan)
+    #from fairseq.debug_tools import check_data_iterator 
+    #check_data_iterator(itr, source_dict=task.source_dictionary, target_dict=task.tgt_dict)
+
     extra_meters = collections.defaultdict(lambda: AverageMeter())
     valid_subsets = args.valid_subset.split(',')
     max_update = args.max_update or math.inf
     for i, samples in enumerate(progress, start=epoch_itr.iterations_in_epoch):
+
+        # sanity check batch
+        # from fairseq.debug_tools import sanity_check_collated_batch
+        # sanity_check_collated_batch(samples[0], task.datasets['train'])
+
         log_output = trainer.train_step(samples)
         if log_output is None:
             continue
-- 
2.20.1 (Apple Git-117)

