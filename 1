[Data directories:]
/dccstor/jzhou1/work/EXP/data/o3_act-states/oracle
/dccstor/jzhou1/work/EXP/data/o3_act-states/processed
/dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24
[Building oracle actions:]
Directory to oracle: /dccstor/jzhou1/work/EXP/data/o3_act-states/oracle already exists --- do nothing.
[Preprocessing data:]
Directory to processed oracle data: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed
and source pre-trained embeddings: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24
already exists --- do nothing.
[Training:]
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, append_eos_to_target=0, apply_tgt_actnode_masks=0, apply_tgt_src_align=1, apply_tgt_vocab_masks=0, arch='transformer_tgt_pointer', attention_dropout=0.0, bert_backprop=False, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, collate_tgt_states=1, cpu=False, criterion='label_smoothed_cross_entropy_pointer', curriculum=0, data='/dccstor/jzhou1/work/EXP/data/o3_act-states/processed', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=512, decoder_input_dim=256, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, emb_dir='/dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24', encode_state_machine=None, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=512, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, find_unused_parameters=False, fix_batches_to_gpus=False, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=40, label_smoothing=0.01, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format='json', log_interval=1000, loss_coef=1.0, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=120, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3584, max_tokens_valid=3584, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_bert_precompute=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', pointer_dist_decoder_selfattn_avg=0, pointer_dist_decoder_selfattn_heads=1, pointer_dist_decoder_selfattn_infer=5, pointer_dist_decoder_selfattn_layers=[4, 5], pretrained_embed_dim=1024, raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/dccstor/jzhou1/work/EXP/exp_o3_roberta-large-top24_act-pos_vmask0_shiftpos0_cattnmask-layerall-head1-focusp0n0_ptr-layer56-head1/models_ep120_seed42', save_interval=1, save_interval_updates=0, seed=42, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=0, shift_pointer_value=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='amr_action_pointer', tbmf_wrapper=False, tensorboard_logdir='/dccstor/jzhou1/work/EXP/exp_o3_roberta-large-top24_act-pos_vmask0_shiftpos0_cattnmask-layerall-head1-focusp0n0_ptr-layer56-head1/models_ep120_seed42', tgt_src_align_focus='p0n0', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir='../fairseq_ext', valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 33256 types
| [actions_nopos] dictionary: 8936 types
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.en
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/valid.en-actions.en.bert
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/valid.en-actions.en.wordpieces
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/valid.en-actions.en.wp2w
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions_nopos
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions_pos
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions.vocab_masks
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions.actnode_masks
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions.src_cursors
TransformerTgtPointerModel(
  (encoder): TransformerEncoder(
    (subspace): Linear(in_features=1024, out_features=256, bias=False)
    (embed_tokens): Embedding(33256, 256, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(8936, 256, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=512, bias=True)
        (fc2): Linear(in_features=512, out_features=256, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
| model transformer_tgt_pointer, criterion LabelSmoothedCrossEntropyPointerCriterion
| num. model params: 21258240 (num. trained: 21258240)
| training on 1 GPUs
| max tokens per GPU = 3584 and max sentences per GPU = None
| no existing checkpoint found /dccstor/jzhou1/work/EXP/exp_o3_roberta-large-top24_act-pos_vmask0_shiftpos0_cattnmask-layerall-head1-focusp0n0_ptr-layer56-head1/models_ep120_seed42/checkpoint_last.pt
| loading train data for epoch 0
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/train.en-actions.en
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/train.en-actions.en.bert
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/train.en-actions.en.wordpieces
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/train.en-actions.en.wp2w
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/train.en-actions.actions_nopos
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/train.en-actions.actions_pos
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/train.en-actions.actions.vocab_masks
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/train.en-actions.actions.actnode_masks
| loaded 36521 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/train.en-actions.actions.src_cursors

Stopped by user


Stopped by user

[Decoding and computing smatch:]
Namespace(append_eos_to_target=0, batch_normalize_reward=False, beam=1, bpe=None, collate_tgt_states=1, cpu=False, criterion='cross_entropy', data='/dccstor/jzhou1/work/EXP/data/o3_act-states/processed', dataset_impl=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, emb_dir='/dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24', force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', gold_annotations=None, gold_episode_ratio=None, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, machine_rules='/dccstor/jzhou1/work/EXP/data/o3_act-states/oracle/train.rules.json', machine_type='AMR', match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='/dccstor/jzhou1/work/EXP/exp_o3_roberta-large-top24_act-pos_vmask0_shiftpos0_cattnmask-layerall-head1-focusp0n0_ptr-layer56-head1/models_ep120_seed42/checkpoint_last.pt', prefix_size=0, print_alignment=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path='/dccstor/jzhou1/work/EXP/exp_o3_roberta-large-top24_act-pos_vmask0_shiftpos0_cattnmask-layerall-head1-focusp0n0_ptr-layer56-head1/models_ep120_seed42/beam1/valid_checkpoint_last', sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='amr_action_pointer', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../fairseq_ext', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 33256 types
| [actions_nopos] dictionary: 8936 types
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.en
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/valid.en-actions.en.bert
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/valid.en-actions.en.wordpieces
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/en_embeddings/roberta_large_top24/valid.en-actions.en.wp2w
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions_nopos
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions_pos
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions.vocab_masks
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions.actnode_masks
| loaded 1368 examples from: /dccstor/jzhou1/work/EXP/data/o3_act-states/processed/valid.en-actions.actions.src_cursors
| loading model(s) from /dccstor/jzhou1/work/EXP/exp_o3_roberta-large-top24_act-pos_vmask0_shiftpos0_cattnmask-layerall-head1-focusp0n0_ptr-layer56-head1/models_ep120_seed42/checkpoint_last.pt
Traceback (most recent call last):
  File "fairseq_ext/generate.py", line 287, in <module>
    cli_main()
  File "fairseq_ext/generate.py", line 283, in cli_main
    main(args)
  File "fairseq_ext/generate.py", line 110, in main
    task=task,
  File "/dccstor/ykt-parse/AMR/jiawei2020/transition-amr-parser/fairseq/fairseq/checkpoint_utils.py", line 155, in load_model_ensemble
    ensemble, args, _task = load_model_ensemble_and_task(filenames, arg_overrides, task)
  File "/dccstor/ykt-parse/AMR/jiawei2020/transition-amr-parser/fairseq/fairseq/checkpoint_utils.py", line 165, in load_model_ensemble_and_task
    raise IOError('Model file not found: {}'.format(filename))
OSError: Model file not found: /dccstor/jzhou1/work/EXP/exp_o3_roberta-large-top24_act-pos_vmask0_shiftpos0_cattnmask-layerall-head1-focusp0n0_ptr-layer56-head1/models_ep120_seed42/checkpoint_last.pt
